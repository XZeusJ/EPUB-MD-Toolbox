#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Markdown Files Merger
å°†æ¯ç§æ‚å¿—çš„å¤šä¸ªMDæ–‡ä»¶æŒ‰å•è¯æ•°é™åˆ¶æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶ï¼Œå¹¶æ ¹æ®æ—¥æœŸèŒƒå›´å‘½å
"""

import os
import re
from pathlib import Path
from datetime import datetime
import argparse

def extract_date_from_filename(filename):
    """
    ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸ
    æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼ï¼š2024.01.06, 2024-01-06, 20240106ç­‰
    
    Args:
        filename (str): æ–‡ä»¶å
        
    Returns:
        datetime or None: æå–åˆ°çš„æ—¥æœŸå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
    """
    # å„ç§æ—¥æœŸæ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
    date_patterns = [
        r'(\d{4})\.(\d{1,2})\.(\d{1,2})',  # 2024.01.06
        r'(\d{4})-(\d{1,2})-(\d{1,2})',   # 2024-01-06
        r'(\d{4})_(\d{1,2})_(\d{1,2})',   # 2024_01_06
        r'(\d{4})(\d{2})(\d{2})',         # 20240106
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            try:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return datetime(year, month, day)
            except ValueError:
                continue
    
    return None

def get_date_range_from_files(md_files):
    """
    ä»æ–‡ä»¶åˆ—è¡¨ä¸­æå–æ—¥æœŸèŒƒå›´
    
    Args:
        md_files (list): MDæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        tuple: (æœ€æ—©æ—¥æœŸ, æœ€æ™šæ—¥æœŸ, æ—¥æœŸæ ¼å¼åŒ–å­—ç¬¦ä¸²)
    """
    dates = []
    
    for md_file in md_files:
        date_obj = extract_date_from_filename(md_file.name)
        if date_obj:
            dates.append(date_obj)
    
    if not dates:
        return None, None, "æœªçŸ¥æ—¥æœŸèŒƒå›´"
    
    dates.sort()
    earliest = dates[0]
    latest = dates[-1]
    
    # æ ¼å¼åŒ–æ—¥æœŸèŒƒå›´
    if earliest == latest:
        date_range = earliest.strftime("%Y.%m.%d")
    else:
        date_range = f"{earliest.strftime('%Y.%m.%d')}-{latest.strftime('%Y.%m.%d')}"
    
    return earliest, latest, date_range

def count_words(text):
    """
    è®¡ç®—æ–‡æœ¬ä¸­çš„å•è¯æ•°ã€‚
    ç®€å•çš„é€šè¿‡ç©ºæ ¼åˆ†å‰²ï¼Œå¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ã€‚
    
    Args:
        text (str): è¾“å…¥æ–‡æœ¬
        
    Returns:
        int: å•è¯æ•°é‡
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å•è¯ï¼ˆå­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ç»„æˆçš„åºåˆ—ï¼‰
    words = re.findall(r'\b\w+\b', text.lower())
    return len(words)

def merge_magazine_files_by_word_limit(magazine_dir, output_dir="merged_magazines", max_words=500000):
    """
    åˆå¹¶å•ä¸ªæ‚å¿—ç›®å½•ä¸‹çš„æ‰€æœ‰MDæ–‡ä»¶ï¼Œç›´æ¥æŒ‰å•è¯æ•°é™åˆ¶åˆ†å·
    
    Args:
        magazine_dir (Path): æ‚å¿—ç›®å½•è·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•
        max_words (int): æ¯ä¸ªæ–‡ä»¶çš„æœ€å¤§å•è¯æ•°é™åˆ¶
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸåˆå¹¶
    """
    magazine_path = Path(magazine_dir)
    if not magazine_path.exists() or not magazine_path.is_dir():
        print(f"âŒ æ‚å¿—ç›®å½•ä¸å­˜åœ¨: {magazine_dir}")
        return False
    
    # è·å–æ‰€æœ‰MDæ–‡ä»¶
    md_files = list(magazine_path.glob("*.md"))
    if not md_files:
        print(f"âš ï¸  {magazine_path.name} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°MDæ–‡ä»¶")
        return False
    
    # æŒ‰æ–‡ä»¶åæ’åºï¼ˆé€šå¸¸åŒ…å«æ—¥æœŸä¿¡æ¯ï¼‰ï¼Œç¡®ä¿åˆå¹¶é¡ºåºæ­£ç¡®
    md_files.sort(key=lambda x: x.name)
    
    print(f"ğŸ“š å¤„ç†æ‚å¿—: {magazine_path.name}")
    print(f"ğŸ“„ æ‰¾åˆ° {len(md_files)} ä¸ªMDæ–‡ä»¶")
    print(f"ğŸ“ æ¯ä¸ªæ–‡ä»¶æœ€å¤§å•è¯æ•°é™åˆ¶: {max_words:,}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    magazine_name = magazine_path.name
    
    try:
        merged_files_info = [] # Stores info about each created merged file

        current_part_content_parts = [] # List of content strings for the current part
        current_part_word_count = 0
        current_part_original_files = [] # List of original Path objects for the current part
        
        print(f"\nå¼€å§‹åˆå¹¶ {magazine_name} çš„æ–‡ä»¶ (ç›®æ ‡å•è¯æ•°: {max_words:,})")

        for k, md_file in enumerate(md_files, 1): # Iterate through all MD files
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                if not content:
                    print(f"    âš ï¸  æ–‡ä»¶ {md_file.name} ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                processed_content = adjust_header_levels(content)
                
                file_date = extract_date_from_filename(md_file.name)
                issue_title_text = file_date.strftime('%Yå¹´%mæœˆ%dæ—¥') if file_date else md_file.stem
                # Use ### for individual issue titles to keep hierarchy below top-level file title
                issue_header = f"\n\n---\n\n### ç¬¬ {k} æœŸ - {issue_title_text}\n" 
                
                article_full_content = issue_header + processed_content
                article_word_count = count_words(article_full_content)

                # Check if adding this article exceeds max_words for the current part
                # And ensure we don't create an empty part if the current one is already empty
                if current_part_word_count > 0 and (current_part_word_count + article_word_count > max_words):
                    # Save current part and start a new one
                    file_info = save_merged_part_file(
                        magazine_name,
                        current_part_content_parts, 
                        current_part_original_files, 
                        output_path,
                        max_words # Pass max_words for display in file header
                    )
                    merged_files_info.append(file_info)
                    print(f"    ğŸ“¦ å·²ç”Ÿæˆæ–‡ä»¶: {file_info['filename']} ({file_info['word_count']:,} å•è¯)")
                    
                    # Reset for new part
                    current_part_content_parts = []
                    current_part_word_count = 0
                    current_part_original_files = []
                    
                # Add current article to current part (or new part)
                current_part_content_parts.append(article_full_content)
                current_part_word_count += article_word_count
                current_part_original_files.append(md_file)
                # The 'part_num' in this print statement is now internal to how many articles are in the current part
                print(f"    âœ… æ·»åŠ æ–‡ä»¶ {md_file.name} åˆ°å½“å‰åˆå¹¶éƒ¨åˆ† ({current_part_word_count:,} å•è¯)")

            except Exception as e:
                print(f"    âŒ è¯»å–æˆ–å¤„ç†æ–‡ä»¶ {md_file.name} å¤±è´¥: {e}")
                continue
        
        # Save the last part (if it has content)
        if current_part_original_files:
            file_info = save_merged_part_file(
                magazine_name,
                current_part_content_parts, 
                current_part_original_files, 
                output_path,
                max_words # Pass max_words for display in file header
            )
            merged_files_info.append(file_info)
            print(f"    ğŸ“¦ å·²ç”Ÿæˆæœ€åéƒ¨åˆ†æ–‡ä»¶: {file_info['filename']} ({file_info['word_count']:,} å•è¯)")
        
        # Output statistics for the magazine
        print(f"\nâœ… {magazine_name} åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“š å…±åˆ›å»º {len(merged_files_info)} ä¸ªåˆå¹¶æ–‡ä»¶")
        
        total_words_in_merged_files = 0
        for info in merged_files_info:
            print(f"  ğŸ“– {info['filename']}: {info['word_count']:,} å•è¯, åŒ…å« {info['file_count']} æœŸ")
            total_words_in_merged_files += info['word_count']
        
        print(f"ğŸ“Š è¯¥æ‚å¿—æ€»å•è¯æ•°: {total_words_in_merged_files:,}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False

def save_merged_part_file(magazine_name, content_parts, original_files, output_path, max_words):
    """
    ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶ã€‚
    æ–‡ä»¶åæ ¼å¼: æ‚å¿—å-YYYY.MM.DD[-YYYY.MM.DD][_partX].md
    å†…éƒ¨æ ‡é¢˜æ ¼å¼: # æ‚å¿—å -YYYY.MM.DD[-YYYY.MM.DD] (ç¬¬ X éƒ¨åˆ†)
    
    Args:
        magazine_name (str): æ‚å¿—åç§°
        content_parts (list): å½“å‰éƒ¨åˆ†çš„å†…å®¹ç‰‡æ®µåˆ—è¡¨
        original_files (list): è¯¥éƒ¨åˆ†åŒ…å«çš„åŸå§‹æ–‡ä»¶Pathå¯¹è±¡åˆ—è¡¨
        output_path (Path): è¾“å‡ºè·¯å¾„
        max_words (int): æœ€å¤§å•è¯é™åˆ¶ (ç”¨äºä¿¡æ¯æ˜¾ç¤º)
        
    Returns:
        dict: åˆå¹¶æ–‡ä»¶ä¿¡æ¯
    """
    
    # è·å–æ—¥æœŸèŒƒå›´ç”¨äºæ–‡ä»¶åå’Œä¿¡æ¯æ˜¾ç¤º
    earliest, latest, date_range_str = get_date_range_from_files(original_files)

    # Clean the magazine name for filename use (replace underscores with hyphens)
    clean_magazine_name = magazine_name.replace('_', '-')

    # æ„å»ºåŸºäºæ—¥æœŸèŒƒå›´çš„åŸºç¡€æ–‡ä»¶åï¼ˆä¸å¸¦partXï¼‰
    if earliest and latest:
        base_filename_no_part = f"{clean_magazine_name}-{date_range_str}"
    else:
        # Fallback if no dates could be extracted, use a generic name
        base_filename_no_part = f"{clean_magazine_name}_merged" 

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŒåçš„æ–‡ä»¶ï¼Œä»¥ç¡®å®šæ˜¯å¦éœ€è¦æ·»åŠ  _partX åç¼€
    # è¿™é‡Œæˆ‘ä»¬åªå…³å¿ƒæ˜¯å¦å­˜åœ¨ä¸å¸¦ _partX çš„æ–‡ä»¶ï¼Œæˆ–è€…å·²ç»å­˜åœ¨çš„ _partX æ–‡ä»¶
    existing_files_with_base_prefix = sorted(list(output_path.glob(f"{base_filename_no_part}*.md")))
    
    actual_part_num = 1 # é»˜è®¤æ˜¯ç¬¬ä¸€éƒ¨åˆ†

    if existing_files_with_base_prefix:
        # å¦‚æœå·²ç»å­˜åœ¨æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°æœ€å¤§çš„ part number
        max_existing_part = 0
        for f in existing_files_with_base_prefix:
            match = re.search(r'_part(\d+)\.md$', f.name)
            if match:
                max_existing_part = max(max_existing_part, int(match.group(1)))
            elif f.name == f"{base_filename_no_part}.md":
                max_existing_part = max(max_existing_part, 1) # åŸºç¡€æ–‡ä»¶åè¢«è®¤ä¸ºæ˜¯ part 1

        actual_part_num = max_existing_part + 1 # æ–°æ–‡ä»¶çš„ part number

    # æ„å»ºæœ€ç»ˆæ–‡ä»¶å
    if actual_part_num > 1:
        filename = f"{base_filename_no_part}_part{actual_part_num}.md"
    else:
        filename = f"{base_filename_no_part}.md"
        
    output_file = output_path / filename

    # ç”Ÿæˆæ–‡ä»¶å†…éƒ¨ä¸»æ ‡é¢˜
    volume_title = f"# {magazine_name.replace('_', ' ').title()}"
    if earliest and latest:
        volume_title += f" - {date_range_str}"
    if actual_part_num > 1: # åªæœ‰å½“å®é™…part_numå¤§äº1æ—¶æ‰åœ¨å†…éƒ¨æ ‡é¢˜ä¸­æ˜¾ç¤º
        volume_title += f" (ç¬¬ {actual_part_num} éƒ¨åˆ†)"
    
    # ç»„è£…å®Œæ•´å†…å®¹
    full_content_list = [
        volume_title,
        f"\n> æœ¬éƒ¨åˆ†åŒ…å« {len(original_files)} æœŸå†…å®¹"
    ]
    if earliest and latest and earliest != latest: # Add date range if it's actually a range
        full_content_list.append(f"> æ–‡ä»¶æ—¥æœŸèŒƒå›´: {date_range_str}")
    
    full_content_list.append(f"> ç›®æ ‡å•è¯æ•°é™åˆ¶: {max_words:,} å•è¯\n")
        
    full_content_list.extend(content_parts)
    
    final_content = "".join(full_content_list)
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(final_content)
    
    return {
        'filename': filename,
        'char_count': len(final_content), 
        'word_count': count_words(final_content), 
        'file_count': len(original_files),
        'part_num': actual_part_num # è¿”å›å®é™…çš„ part_num
    }

def adjust_header_levels(content):
    """
    è°ƒæ•´Markdownå†…å®¹ä¸­çš„æ ‡é¢˜å±‚çº§ï¼Œé¿å…ä¸ä¸»æ ‡é¢˜å†²çª
    å°†åŸæœ‰çš„ # è½¬ä¸º ###ï¼Œ## è½¬ä¸º ####ï¼Œä»¥æ­¤ç±»æ¨
    
    Args:
        content (str): åŸå§‹å†…å®¹
        
    Returns:
        str: è°ƒæ•´åçš„å†…å®¹
    """
    lines = content.split('\n')
    adjusted_lines = []
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
        if line.strip().startswith('#'):
            # è®¡ç®—åŸæœ‰çš„#æ•°é‡
            hash_count = 0
            for char in line:
                if char == '#':
                    hash_count += 1
                else:
                    break
            
            # è°ƒæ•´æ ‡é¢˜å±‚çº§ï¼šåŸæ¥çš„#å˜æˆ###ï¼ˆå‘ä¸‹ç§»åŠ¨2çº§ï¼‰
            adjusted_level = hash_count + 2
            title_text = line[hash_count:].strip()
            adjusted_line = '#' * adjusted_level + ' ' + title_text
            adjusted_lines.append(adjusted_line)
        else:
            adjusted_lines.append(line)
    
    return '\n'.join(adjusted_lines)

def merge_all_magazines(input_dir="converted_md", output_dir="merged_magazines", max_words=500000):
    """
    æ‰¹é‡åˆå¹¶æ‰€æœ‰æ‚å¿—çš„MDæ–‡ä»¶ï¼Œç›´æ¥æŒ‰å•è¯æ•°é™åˆ¶åˆ†å·
    
    Args:
        input_dir (str): è¾“å…¥ç›®å½•ï¼ˆåŒ…å«å„æ‚å¿—å­ç›®å½•ï¼‰
        output_dir (str): è¾“å‡ºç›®å½•
        max_words (int): æ¯ä¸ªæ–‡ä»¶çš„æœ€å¤§å•è¯æ•°é™åˆ¶
    """
    input_path = Path(input_dir)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # è·å–æ‰€æœ‰æ‚å¿—ç›®å½•
    magazine_dirs = [d for d in input_path.iterdir() if d.is_dir()]
    
    if not magazine_dirs:
        print(f"âŒ åœ¨ {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æ‚å¿—ç›®å½•")
        return
    
    print(f"ğŸ” æ‰¾åˆ° {len(magazine_dirs)} ä¸ªæ‚å¿—ç›®å½•")
    print(f"ğŸ“ å…¨å±€è®¾ç½®ï¼šæ¯ä¸ªæ–‡ä»¶æœ€å¤§å•è¯æ•°é™åˆ¶: {max_words:,}") 
    
    success_count = 0
    total_files_processed = 0 
    total_merged_volumes = 0 
    
    for magazine_dir in magazine_dirs:
        print(f"\n{'='*70}")
        # Call the new merge function
        if merge_magazine_files_by_word_limit(magazine_dir, output_dir, max_words):
            success_count += 1
        
        md_count = len(list(magazine_dir.glob("*.md")))
        total_files_processed += md_count
        
        output_path = Path(output_dir)
        # Match files that start with the clean magazine name and end with .md
        clean_magazine_name = magazine_dir.name.replace('_', '-')
        magazine_merged_files = [f for f in output_path.glob(f"{clean_magazine_name}-*.md") if f.is_file()]
        total_merged_volumes += len(magazine_merged_files)
    
    print(f"\n{'='*70}")
    print("ğŸ‰ æ‰¹é‡åˆå¹¶å®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸå¤„ç† {success_count}/{len(magazine_dirs)} ä¸ªæ‚å¿—")
    print(f"ğŸ“„ æ€»è®¡å¤„ç† {total_files_processed} ä¸ªåŸå§‹MDæ–‡ä»¶")
    print(f"ğŸ“š æ€»è®¡ç”Ÿæˆ {total_merged_volumes} ä¸ªåˆå¹¶æ–‡ä»¶")
    print(f"ğŸ“ æ‰€æœ‰åˆå¹¶åçš„æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}/")
    print(f"ğŸ’¡ æ¯ä¸ªæ–‡ä»¶å•è¯æ•°å°†é™åˆ¶åœ¨ {max_words:,} å•è¯ä»¥å†…ï¼Œå¹¶æ ¹æ®æ—¥æœŸèŒƒå›´å‘½åã€‚") 

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Markdownæ–‡ä»¶åˆå¹¶å™¨ï¼ˆæŒ‰å•è¯æ•°é™åˆ¶åˆ†å·ï¼Œå¹¶æ ¹æ®æ—¥æœŸèŒƒå›´å‘½åï¼‰") 
    parser.add_argument("input", nargs="?", default="converted_md",
                       help="è¾“å…¥ç›®å½• (é»˜è®¤: converted_md)")
    parser.add_argument("-o", "--output", default="merged_magazines",
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: merged_magazines)")
    parser.add_argument("--magazine", 
                       help="åªåˆå¹¶æŒ‡å®šçš„æ‚å¿—ç›®å½•")
    parser.add_argument("--max-words", type=int, default=500000, 
                       help="æ¯ä¸ªæ–‡ä»¶çš„æœ€å¤§å•è¯æ•°é™åˆ¶ (é»˜è®¤: 500000)") 
    
    args = parser.parse_args()
    
    print("ğŸ“š Markdownæ–‡ä»¶åˆå¹¶å™¨ï¼ˆæŒ‰å•è¯æ•°é™åˆ¶åˆ†å·ï¼Œå¹¶æ ¹æ®æ—¥æœŸèŒƒå›´å‘½åï¼‰") 
    
    if args.magazine:
        magazine_path = Path(args.input) / args.magazine
        merge_magazine_files_by_word_limit(magazine_path, args.output, args.max_words) 
    else:
        merge_all_magazines(args.input, args.output, args.max_words) 

if __name__ == "__main__":
    main()
